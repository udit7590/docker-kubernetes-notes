--------------------------------------------------------------------------------------------------
> THE BSAICS
--------------------------------------------------------------------------------------------------
  > History
    - Docker till 1.12 didn't have anything excited for networking. They bought SocketPlace and then Docker started having a scalable Networking feature
  > 3 Pillars of Docker Networking
    > CNM (Container Network Model)
      - It's a specification
      - Grand Design/DNA of Docker Networking
      - Rival - CNI (Container Network Interface) -> More suited for Kubernetes environments
      - 3 components
        : Sandbox - Aka Namespace, Isolated area of OS
          - Contains full network stack
        : Endpoint
          - Network Interface Eg. eth0
        : Network
          - Connected Endpoints

    > Libnetwork
      - Real World implementation of CNM by Docker Inc
      - Written in Go/Golang
      - X-Platform
      - https://github.com/docker/libnetwork
      - Central place for all Docker Networking logic, API, UX, etc...
      - Pluggable
      - Control Plane & Management Plane
    
    > Drivers
      - Network-specific detail
        : Overlay
        : MACVLAN
        : IPVLAN
        : Bridge
      - Data plane
      - Loval (or native) vs Remote (or 3rd party drivers)

  > Hands-On
      -----------------------
    > Basic Commands
      -----------------------
      - docker network
        # Help
      - docker network ls
        # Network ID, NAME, Driver, Scope
        # Scope can be Local or Swarm
      - docker network inspect bridge
      - docker info
        # In the Network - you'll see the list of network drivers available

--------------------------------------------------------------------------------------------------
> USE CASES & DRIVERS
--------------------------------------------------------------------------------------------------
  > Single-host Networking
    With the bridge driver
    : Containers on different host cannot talk to each other
    : 802.1d bridge
      aka virtual switch
      aka vswitch
    : Kernel feature on Linux - mature and stable, fast

    - docker network create -d bridge --subnet 10.0.0.1/24 ps-bridge
    - docker network inspect ps-bridge
    - sudo apt-get install bridge-utils
      brctl # Shows kernel bridges on the host
      ip link show # Also shows details of bridges
    - docker run -dt --name c1 --network ps-bridge alpine slep 1d
      -t = terminal
      brctl show # Now the bridges have interfaces which would actually be our containers

  > Multi Host Overlay Networking
    With Docker overlay driver
    : Multiple containers over different nodes can talk directly to each other (over layer 2) using tunnels
      - Layer 2 Adjacency
    : Router is not even aware of it. Although Router is still used to pass the data, effectively the communication is at Layer 2 of Networking

    - docker network create -d overlap <overlay-name>

  > Participating in existing networks
    With the MACVLAN driver
    : Every container gets its own IP
    : Every container gets its own MAC
    - Cant be tested on AWS providers

  > Participating in existing networks
    With the Linux IPVLAN driver
    - Doesnt give containers their own MAC addresses
    - More cloud friendly
    - Special considerations when working with DHCPs (since DHCP is designed to work with MAC addresses)
      : A single MAC address will be asking for multiple IP addresses

--------------------------------------------------------------------------------------------------
> NETWORK SERVICES
--------------------------------------------------------------------------------------------------
  > Service Discovery
    - Service Discovery is automatic in Docker
      : For services
      : For containers created with --name or --alias
    - Every container gets a small DNS resolver
      : listens on 127.0.0.11:53
      : Forwards requests to DNS server on Docker host
    - Network scoped

  > The Routing Mesh (Layer 4/Transport Layer)
  > The HTTP Routing Mesh - HRM (Layer 7/Application Layer)
    - The Routing Mesh does not know about anything that goes to HTTP layer or Layer 7
      - HRM Layer fills the gap for this
    - Requires Docker Datacenter
    - ucp-hrm (overlay)


-> Didn't pay too much attention here. Better course should be CNI in Kubernetes





